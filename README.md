# Kindle-Reviews-Sentiment-Analysis-using-BiLSTM
End-to-end NLP pipeline for binary sentiment classification on Amazon Kindle reviews using TensorFlow, BiLSTM, Embedding, and a fully-balanced training setup. This project delivers a robust text-classification system with clean preprocessing, model training, evaluation, and error analysis â€” tuned for operational excellence and scalable deployment.

ğŸš€ Project Overview

This repo demonstrates how to build a sentiment classifier that predicts whether a Kindle review is positive (>3 stars) or negative (â‰¤3 stars).

The full pipeline covers:

ğŸ§¹ Data cleaning & preprocessing

ğŸ§¬ Text tokenization + padded sequences

âš–ï¸ Upsampling to handle class imbalance

ğŸ§  BiLSTM-based deep learning architecture

ğŸ“Š Model performance metrics: Accuracy, ROC-AUC, AUPRC

ğŸ” Misclassification analysis for model insights

The system ships with 83% accuracy, solid ROC-AUC performance, and deploy-ready training assets like checkpoints & callbacks.

ğŸ“‚ Repository Structure
ğŸ“ kindle-sentiment-bilstm
 â”œâ”€â”€ all_kindle_review.csv         # Dataset
 â”œâ”€â”€ sentiment_model.ipynb         # Full training notebook
 â”œâ”€â”€ best_rnn_binary.h5            # Saved best model weights
 â”œâ”€â”€ tokenizer_config.pkl          # (optional) Tokenizer object
 â”œâ”€â”€ README.md                     # This file
 â””â”€â”€ /plots                        # ROC, PR curve, calibration plots

ğŸ§¼ Data Preprocessing Workflow

The pipeline applies a structured text-cleaning strategy:

Lowercasing

Removing HTML & non-alphanumeric noise

Combining summary + reviewText

Null removal

Labeling rule:

rating <= 3 â†’ 0 (Negative)

rating > 3 â†’ 1 (Positive)

A custom cleaning function ensures enterprise-grade consistency across text inputs.

ğŸ§± Model Architecture

A lean yet powerful sequential architecture optimized for textual signal extraction:

Embedding (30k vocab, 128 dims)
BiLSTM (128 units)
Dropout (0.3)
Dense (64, ReLU)
Dropout (0.2)
Dense (1, Sigmoid)


Hyperparameters optimized for:

Lower generalization error

Stability across imbalanced datasets

Minimal overfitting (via EarlyStopping + ModelCheckpoint)

ğŸ“Š Model Performance (Test Set)
Metric	Score
Accuracy	0.8319
Precision	0.83
Recall	0.83
F1 Score	0.83
ROC-AUC	0.8319
AUPRC	0.7756

The model demonstrates strong calibration and balanced class performance.

ğŸ“ˆ Evaluation Visuals

The project includes:

ROC Curve â€” AUC ~0.83

Precision-Recall Curve â€” AUPRC ~0.77

Calibration Curve â€” probability reliability

Error Analysis â€” top misclassified samples for root-cause investigation

ğŸ§ª Training Strategy
Balanced Learning

Applied upsampling to ensure equal representation of both sentiment classes in training.

Callbacks Used

EarlyStopping (patience=3)

ModelCheckpoint (best weights)

Training Statistics

Epochs: 12

Batch size: 64

Optimizer: Adam (lr=2e-4)

ğŸ” Error Analysis

The script prints out misclassified rows along with:

True label

Predicted label

Model probability

Reconstructed text from padded sequence

This creates full transparency for evaluating model blind spots.

ğŸ›  Technologies Used

Python 3.x

TensorFlow / Keras

Scikit-Learn

NLTK

NumPy + Pandas

Matplotlib

ğŸ“¦ How to Run
pip install -r requirements.txt
python sentiment_training.py


Or open sentiment_model.ipynb in Jupyter/Colab.

ğŸ¤ Contributing

PRs are welcome. If you want to scale this model into production-grade microservices (FastAPI + Docker + AWS), feel free to collaborate.

â­ Future Enhancements

Switch to GRU or Transformer Encoder

Integrate GloVe/Word2Vec embeddings

Deploy REST API using FastAPI

Add explainability using LIME/SHAP

ğŸ Final Thoughts

This repository is a streamlined, data-first implementation showcasing clean NLP engineering pipelines with practical deep-learning insights. Ideal for interview prep, portfolio display, or production prototyping.
